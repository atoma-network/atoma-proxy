openapi: 3.1.0
info:
  title: atoma-proxy
  description: ''
  license:
    name: Apache-2.0
  version: 0.1.0
servers:
- url: http://localhost:8080
paths:
  /health:
    get:
      tags:
      - Health
      summary: Health
      operationId: health
      responses:
        '200':
          description: Service is healthy
          content:
            application/json:
              schema: {}
        '500':
          description: Service is unhealthy
  /node/registration:
    post:
      tags:
      - Node Public Address Registration
      summary: Register node
      description: |-
        This endpoint allows nodes to register or update their public address in the system.
        When a node comes online or changes its address, it can use this endpoint to ensure
        the system has its current address for routing requests.

        # Arguments

        * `state` - The shared application state containing the state manager sender
        * `payload` - The registration payload containing the node's ID and public address

        # Returns

        Returns `Ok(Json(Value::Null))` on successful registration, or an error status code
        if the registration fails.

        # Errors

        Returns `StatusCode::INTERNAL_SERVER_ERROR` if:
        * The state manager channel is closed
        * The registration event cannot be sent

        # Example Request Payload

        ```json
        {
            "node_small_id": 123,
            "public_address": "http://node-123.example.com:8080"
        }
        ```
      operationId: node_public_address_registration
      responses:
        '200':
          description: Node public address registered successfully
          content:
            application/json:
              schema: {}
        '500':
          description: Failed to register node public address
  /v1/chat/completions:
    post:
      tags:
      - Chat Completions
      summary: Create chat completion
      description: |-
        This function processes chat completion requests by determining whether to use streaming
        or non-streaming response handling based on the request payload. For streaming requests,
        it configures additional options to track token usage.

        # Arguments

        * `metadata`: Extension containing request metadata (node address, ID, compute units, etc.)
        * `state`: The shared state of the application
        * `headers`: The headers of the request
        * `payload`: The JSON payload containing the chat completion request

        # Returns

        Returns a Response containing either:
        - A streaming SSE connection for real-time completions
        - A single JSON response for non-streaming completions

        # Errors

        Returns an error status code if:
        - The request processing fails
        - The streaming/non-streaming handlers encounter errors
        - The underlying inference service returns an error
      operationId: chat_completions_handler
      requestBody:
        content:
          application/json:
            schema:
              $ref: '#/components/schemas/ChatCompletionRequest'
        required: true
      responses:
        '200':
          description: Chat completions
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/ChatCompletionResponse'
        '400':
          description: Bad request
        '401':
          description: Unauthorized
        '500':
          description: Internal server error
  /v1/embeddings:
    post:
      tags:
      - Embeddings
      summary: Create embeddings
      description: |-
        This endpoint follows the OpenAI API format for generating vector embeddings from input text.
        The handler receives pre-processed metadata from middleware and forwards the request to
        the selected node.

        Note: Authentication, node selection, and initial request validation are handled by middleware
        before this handler is called.

        # Arguments
        * `metadata` - Pre-processed request metadata containing node information and compute units
        * `state` - The shared proxy state containing configuration and runtime information
        * `headers` - HTTP headers from the incoming request
        * `payload` - The JSON request body containing the model and input text

        # Returns
        * `Ok(Response)` - The embeddings response from the processing node
        * `Err(StatusCode)` - An error status code if any step fails

        # Errors
        * `INTERNAL_SERVER_ERROR` - Processing or node communication failures
      operationId: embeddings_handler
      requestBody:
        content:
          application/json:
            schema: {}
        required: true
      responses:
        '200':
          description: Embeddings generated successfully
          content:
            application/json:
              schema: {}
        '400':
          description: Bad request
        '401':
          description: Unauthorized
        '500':
          description: Internal server error
  /v1/images/generations:
    post:
      tags:
      - Image Generations
      summary: Create image generation
      description: |-
        This endpoint processes requests to generate images using AI models by forwarding them
        to the appropriate AI node. The request metadata and compute units have already been
        validated by middleware before reaching this handler.

        # Arguments
        * `metadata` - Extension containing pre-processed request metadata (node address, compute units, etc.)
        * `state` - Application state containing configuration and shared resources
        * `headers` - HTTP headers from the incoming request
        * `payload` - JSON payload containing image generation parameters

        # Returns
        * `Result<Response<Body>, StatusCode>` - The processed response from the AI node or an error status

        # Errors
        * Returns various status codes based on the underlying `handle_image_generation_response`:
          - `INTERNAL_SERVER_ERROR` - If there's an error communicating with the AI node

        # Example Payload
        ```json
        {
            "model": "stable-diffusion-v1-5",
            "n": 1,
            "size": "1024x1024"
        }
        ```
      operationId: image_generations_handler
      requestBody:
        content:
          application/json:
            schema: {}
        required: true
      responses:
        '200':
          description: Image generations
          content:
            application/json:
              schema: {}
        '400':
          description: Bad request
        '401':
          description: Unauthorized
        '500':
          description: Internal server error
  /v1/models:
    get:
      tags:
      - Models
      summary: List models
      description: |-
        This endpoint mimics the OpenAI models endpoint format, returning a list of
        available models with their associated metadata and permissions. Each model
        includes standard OpenAI-compatible fields to ensure compatibility with
        existing OpenAI client libraries.

        # Arguments

        * `state` - The shared application state containing the list of available models

        # Returns

        Returns a JSON response containing:
        * An "object" field set to "list"
        * A "data" array containing model objects with the following fields:
          - id: The model identifier
          - object: Always set to "model"
          - created: Timestamp (currently hardcoded)
          - owned_by: Set to "atoma"
          - root: Same as the model id
          - parent: Set to null
          - max_model_len: Maximum context length (currently hardcoded to 2048)
          - permission: Array of permission objects describing model capabilities

        # Example Response

        ```json
        {
          "object": "list",
          "data": [
            {
              "id": "meta-llama/Llama-3.1-70B-Instruct",
              "object": "model",
              "created": 1730930595,
              "owned_by": "atoma",
              "root": "meta-llama/Llama-3.1-70B-Instruct",
              "parent": null,
              "max_model_len": 2048,
              "permission": [
                {
                  "id": "modelperm-meta-llama/Llama-3.1-70B-Instruct",
                  "object": "model_permission",
                  "created": 1730930595,
                  "allow_create_engine": false,
                  "allow_sampling": true,
                  "allow_logprobs": true,
                  "allow_search_indices": false,
                  "allow_view": true,
                  "allow_fine_tuning": false,
                  "organization": "*",
                  "group": null,
                  "is_blocking": false
                }
              ]
            }
          ]
        }
        ```
      operationId: models_handler
      responses:
        '200':
          description: List of available models
          content:
            application/json:
              schema: {}
        '500':
          description: Failed to retrieve list of available models
components:
  schemas:
    ChatCompletionChoice:
      type: object
      required:
      - index
      - message
      properties:
        finish_reason:
          type:
          - string
          - 'null'
          description: The reason the chat completion was finished.
        index:
          type: integer
          format: int32
          description: The index of this choice in the list of choices.
        logprobs:
          description: Log probability information for the choice, if applicable.
        message:
          $ref: '#/components/schemas/ChatCompletionMessage'
          description: The chat completion message.
    ChatCompletionChunk:
      type: object
      required:
      - id
      - created
      - model
      - choices
      properties:
        choices:
          type: array
          items:
            $ref: '#/components/schemas/ChatCompletionChunkChoice'
          description: A list of chat completion chunk choices.
        created:
          type: integer
          format: int64
          description: The Unix timestamp (in seconds) of when the chunk was created.
        id:
          type: string
          description: A unique identifier for the chat completion chunk.
        model:
          type: string
          description: The model used for the chat completion.
    ChatCompletionChunkChoice:
      type: object
      required:
      - index
      - delta
      properties:
        delta:
          $ref: '#/components/schemas/ChatCompletionChunkDelta'
          description: The chat completion delta message for streaming.
        finish_reason:
          type:
          - string
          - 'null'
          description: The reason the chat completion was finished, if applicable.
        index:
          type: integer
          format: int32
          description: The index of this choice in the list of choices.
    ChatCompletionChunkDelta:
      type: object
      properties:
        content:
          type:
          - string
          - 'null'
          description: The content of the message, if present in this chunk.
        function_call:
          description: The function call information, if present in this chunk.
        role:
          type:
          - string
          - 'null'
          description: The role of the message author, if present in this chunk.
        tool_calls:
          type:
          - array
          - 'null'
          items: {}
          description: The tool calls information, if present in this chunk.
    ChatCompletionMessage:
      type: object
      required:
      - role
      - content
      properties:
        content:
          type: string
          description: The contents of the message
        name:
          type:
          - string
          - 'null'
          description: The name of the author of this message
        role:
          type: string
          description: 'The role of the message author. One of: "system", "user", "assistant", "tool", or "function"'
    ChatCompletionRequest:
      type: object
      required:
      - model
      - messages
      properties:
        frequency_penalty:
          type:
          - number
          - 'null'
          format: float
          description: |-
            Number between -2.0 and 2.0. Positive values penalize new tokens based on their
            existing frequency in the text so far
        function_call:
          description: Controls how the model responds to function calls
        functions:
          type:
          - array
          - 'null'
          items: {}
          description: A list of functions the model may generate JSON inputs for
        logit_bias:
          type:
          - object
          - 'null'
          description: Modify the likelihood of specified tokens appearing in the completion
          additionalProperties:
            type: number
            format: float
          propertyNames:
            type: string
        max_tokens:
          type:
          - integer
          - 'null'
          format: int32
          description: The maximum number of tokens to generate in the chat completion
        messages:
          type: array
          items:
            $ref: '#/components/schemas/ChatCompletionMessage'
          description: A list of messages comprising the conversation so far
        model:
          type: string
          description: ID of the model to use
        n:
          type:
          - integer
          - 'null'
          format: int32
          description: How many chat completion choices to generate for each input message
        presence_penalty:
          type:
          - number
          - 'null'
          format: float
          description: |-
            Number between -2.0 and 2.0. Positive values penalize new tokens based on
            whether they appear in the text so far
        response_format:
          description: The format to return the response in
        seed:
          type:
          - integer
          - 'null'
          format: int64
          description: If specified, our system will make a best effort to sample deterministically
        stop:
          type:
          - array
          - 'null'
          items:
            type: string
          description: Up to 4 sequences where the API will stop generating further tokens
        stream:
          type:
          - boolean
          - 'null'
          description: Whether to stream back partial progress
        temperature:
          type:
          - number
          - 'null'
          format: float
          description: What sampling temperature to use, between 0 and 2
        tool_choice:
          description: Controls which (if any) tool the model should use
        tools:
          type:
          - array
          - 'null'
          items: {}
          description: A list of tools the model may call
        top_p:
          type:
          - number
          - 'null'
          format: float
          description: An alternative to sampling with temperature
        user:
          type:
          - string
          - 'null'
          description: A unique identifier representing your end-user
    ChatCompletionResponse:
      type: object
      required:
      - id
      - created
      - model
      - choices
      properties:
        choices:
          type: array
          items:
            $ref: '#/components/schemas/ChatCompletionChoice'
          description: A list of chat completion choices.
        created:
          type: integer
          format: int64
          description: The Unix timestamp (in seconds) of when the chat completion was created.
        id:
          type: string
          description: A unique identifier for the chat completion.
        model:
          type: string
          description: The model used for the chat completion.
        system_fingerprint:
          type:
          - string
          - 'null'
          description: The system fingerprint for the completion, if applicable.
        usage:
          oneOf:
          - type: 'null'
          - $ref: '#/components/schemas/CompletionUsage'
            description: Usage statistics for the completion request.
    CompletionUsage:
      type: object
      required:
      - prompt_tokens
      - completion_tokens
      - total_tokens
      properties:
        completion_tokens:
          type: integer
          format: int32
          description: Number of tokens in the completion.
        prompt_tokens:
          type: integer
          format: int32
          description: Number of tokens in the prompt.
        total_tokens:
          type: integer
          format: int32
          description: Total number of tokens used (prompt + completion).
tags:
- name: Health
  description: Health check
- name: Chat Completions
  description: Chat completions
- name: Models
  description: Models
- name: Node Public Address Registration
  description: Node public address registration
- name: Embeddings
  description: OpenAI's API embeddings v1 endpoint
- name: Image Generations
  description: OpenAI's API image generations v1 endpoint
